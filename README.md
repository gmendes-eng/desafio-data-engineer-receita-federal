Desafio de Engenharia de Dados - Pipeline da Receita Federal
Este repositÃ³rio contÃ©m a soluÃ§Ã£o para o desafio de engenharia de dados, que consiste em construir um pipeline para ingestÃ£o, processamento e armazenamento de dados pÃºblicos de CNPJ da Receita Federal.

ğŸ“ DescriÃ§Ã£o do Projeto
O pipeline foi construÃ­do utilizando a arquitetura Medallion (Bronze, Silver, Gold) para garantir a qualidade e a rastreabilidade dos dados. O processo completo Ã© orquestrado com Docker e Docker Compose, permitindo que todo o ambiente seja executado com um Ãºnico comando.

Camada Bronze: IngestÃ£o dos dados brutos (.zip) da fonte oficial.

Camada Silver: Limpeza, padronizaÃ§Ã£o de schemas, conversÃ£o de tipos e armazenamento em formato Parquet.

Camada Gold: AplicaÃ§Ã£o das regras de negÃ³cio solicitadas no desafio (cÃ¡lculo de quantidade de sÃ³cios, flags, etc.), com o resultado salvo em Parquet e carregado em um banco de dados.

ğŸ› ï¸ Tecnologias Utilizadas
Linguagem: Python 3.12

Processamento de Dados: Apache Spark (via PySpark)

Banco de Dados: PostgreSQL

OrquestraÃ§Ã£o e Ambiente: Docker e Docker Compose


ğŸš€ Como Executar o Projeto
PrÃ©-requisitos:

Git

Docker e Docker Compose instalados e em execuÃ§Ã£o.

Passo a Passo:

Clone o repositÃ³rio:

git clone https://github.com/gmendes-eng/desafio-data-engineer-receita-federal.git

Navegue atÃ© a pasta do projeto:

cd desafio-data-engineer-receita-federal

Execute o pipeline com Docker Compose:

docker-compose up --build

Este Ãºnico comando irÃ¡ construir a imagem da aplicaÃ§Ã£o, baixar todas as dependÃªncias, iniciar o container do banco de dados e executar o pipeline completo de ponta a ponta. O processo pode levar alguns minutos na primeira execuÃ§Ã£o.

âœ… Como Verificar o Resultado
ApÃ³s a execuÃ§Ã£o bem-sucedida, vocÃª pode verificar o resultado de duas formas:

1. Arquivos FÃ­sicos:
As saÃ­das de cada camada sÃ£o salvas na pasta data/, permitindo auditoria e validaÃ§Ã£o:

data/bronze: ContÃ©m os dados brutos descompactados.

data/silver: ContÃ©m os dados limpos em formato Parquet.

data/gold: ContÃ©m o resultado final em formato Parquet.

2. Banco de Dados PostgreSQL:
O resultado final Ã© carregado na tabela resultado_final_desafio. VocÃª pode usar um cliente de banco de dados (como DBeaver ou DataGrip) para se conectar com as seguintes credenciais:

Host: localhost

Porta: 5432

Banco de Dados: stone_db

UsuÃ¡rio: user

Senha: password

 Desenho da Arquitetura da SoluÃ§Ã£o

 O diagrama abaixo ilustra o fluxo completo, desde a fonte dos dados atÃ© o armazenamento final:

 graph TD;
    subgraph "Fonte de Dados Externa"
        A[ğŸŒ<br>Portal de Dados Abertos<br>Receita Federal<br>(Arquivos .zip)]
    end

    subgraph "Ambiente Dockerizado (docker-compose)"
        subgraph "Container da AplicaÃ§Ã£o (app_stone)"
            B(ğŸ<br>Script de IngestÃ£o<br>ingestion.py)
            
            subgraph "Camadas (Sistema de Arquivos do Container)"
                C(ğŸ¥‰<br>Camada Bronze<br>data/bronze<br>CSVs brutos)
                D(ğŸ¥ˆ<br>Camada Silver<br>data/silver<br>Parquet limpo)
                E(ğŸ¥‡<br>Camada Gold<br>data/gold<br>Parquet agregado)
            end

            F(âœ¨<br>Processamento com PySpark<br>transformations.py)
        end

        subgraph "Container do Banco de Dados (db_stone)"
            G[ğŸ˜<br>PostgreSQL<br>Tabela 'resultado_final_desafio']
        end
    end

    subgraph "Consumidores Finais"
        H(ğŸ“ˆ<br>Ferramenta de BI / API<br>Acessando o PostgreSQL)
    end

    %% Fluxo do Pipeline
    A -->|"1. Download e<br>DescompactaÃ§Ã£o"| B;
    B -->| "2. Salva bruto" | C;
    C -->| "3. LÃª bruto" | F;
    F -->| "4. Salva limpo" | D;
    D -->| "5. LÃª limpo" | F;
    F -->| "6. Salva agregado" | E;
    E -->| "7. LÃª agregado para carregar"| F;
    F -->| "8. Carrega no Banco" | G;
    G -->| "9. Consulta" | H;
